services:

  # frontend-streamlit:
  #   container_name: frontend-streamlit
  #   build: ./app/llama_sensei/frontend
  #   ports:
  #     - ${FRONTEND_PORT}:${FRONTEND_PORT}
  #   environment:
  #     - BACKEND_URL=http://backend-api:8080
  #     - BACKEND_URL=http://backend-api:8080
  #   depends_on:
  #     - "course_management_api"
  #     - "chat_api"
  #   command: ["streamlit", "run", "UI.py", "--server.port", "${FRONTEND_PORT}"]


  chat_api:
    container_name: chat_api
    build: ./app/llama_sensei/backend/qa
    env_file:
      - .env
    ports:
      - ${CHAT_FASTAPI_PORT}:${CHAT_FASTAPI_PORT}
    environment:
      - COURSE_API_URL=http://course_management_api:${COURSE_FASTAPI_PORT}
    depends_on:
      - "course_management_api"


  course_management_api:
    container_name: course_management_api
    build: ./app/llama_sensei/backend/add_courses
    env_file:
      - .env
    ports:
      - ${COURSE_FASTAPI_PORT}:${COURSE_FASTAPI_PORT}
    volumes:
      - ./data:/app/data


networks:
  default:
    name: llama-sensei
